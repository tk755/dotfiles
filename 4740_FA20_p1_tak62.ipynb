{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4740_FA20_p1_tak62.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TusharK54/dotfiles/blob/master/4740_FA20_p1_tak62.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMR2in3Elmb0"
      },
      "source": [
        "# Project 1: Language Modeling and Fake News Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nw061uMf-vyu"
      },
      "source": [
        "Name: Tushar Khan\n",
        "\n",
        "Netid: tak62\n",
        "\n",
        "**After you make your own copy, please rename this notebook by clicking on it's name in the upper left corner.** It should be named: 4740_FA20_p1_netid1_netid2\n",
        "\n",
        "Don't forget to share your newly copied notebook with your partner!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e1Dnqk6GmdH"
      },
      "source": [
        "**Reminder: both of you can't work in this notebook at the same time from different computers/browser windows because of sync issues. We even suggest to close the tab with this notebook when you are not working on it so your partner doesn't get sync issues.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVNUIk8d-oBn"
      },
      "source": [
        "## Introduction\n",
        "In this project we will build an **n-gram-based language model** for fake news classification. We will also investigate a feature-based **Naive Bayes model**. The task we are faced with is to **decide whether a news article is fake or real**. While some fake articles are so absurd and nonsensical that one can clearly guess they are fake, most fake news is actually quite hard to detect. [Various studies](https://pacscenter.stanford.edu/research/program-on-democracy-and-the-internet/deception-detection-accuracy-for-fake-news-headlines-on-social-media/) have shown that most people can have an error rate up to 50% depending on the theme of the article. \n",
        "\n",
        "To help us approach this problem, we will use NLP techniques covered thus far to frame this as a (supervised) binary classification task, where each article will have a label $y \\in \\{0,1\\}$, where *0 indicates a fake article* and *1 indicates a real one*. You will train and validate your two different models and then run them on a test data set with hidden $y$ labels. You will then submit the results on the test data set to Kaggle to participate in our class-wide competition!\n",
        "\n",
        "The project is divided into six parts:\n",
        "1. Dataset loading and preprocessing\n",
        "2. Unsmoothed n-gram language model (LM): build the unsmoothed n-gram language model using our Fake News corpus. \n",
        "3. Smoothed n-gram language model: build a smoothed version of the model from part 2.\n",
        "4. Perplexity: compute perplexity for both the unsmoothed and smoothed model\n",
        "5. Putting everything together and submitting the first model to Kaggle\n",
        "6. Naive Bayes: build a feature-based Naive Bayes model to perform the same classification task. Compare the LM with Naive Bayes and identify the pros and cons of each.\n",
        "\n",
        "**Logistics:** You should work in **groups of 2 students**. Students in the same group will get the same grade. Thus, you should make sure that everyone in your group contributes to the project. \n",
        "- **Remember to form groups on BOTH CMS and Gradescope** or not all group members will receive grades. You can use the Teammate Search option on Piazza to find a partner for this project.\n",
        "\n",
        "**Advice:** Please complete the written parts of this notebook in a clear and informative way. This is where you get to show us that you understand not only what you are doing but also why and how you are doing it. So be clear, organized and concise; avoid vagueness and excess verbiage. Spend time\n",
        "doing error analysis for the models. This is how you understand the advantages and drawbacks of the systems you build. \n",
        "- It's also useful to think about how the theory of n-grams/Naive Bayes bridges with the real world application we are building. Think about what you expect from these models based on your current understanding, and then see if your expectation aligns with empirical results that you'll get. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwgqdDbTPVeA"
      },
      "source": [
        "## General Guidelines\n",
        "In this project, we provide a few code snippets or starter points in case you need them. You DO NOT need to follow the structure. \n",
        "\n",
        "If you think you have a better idea, go for it. You can ADD, MODIFY, or DELETE any code snippets given to you.\n",
        "\n",
        "**Let's do this** ðŸš€"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLmJ-2h5pr6n"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "You are given a **News Corpus** on CMS, which consists of roughly the same amount of real and fake news articles.\n",
        "\n",
        "Real news example:\n",
        "```\n",
        "The OpenAI technology, known as GPT-2, is designed to predict the next word given all the previous words it is shown within some text. The language-based model has been trained on a dataset of 8 million web pages.\n",
        "```\n",
        "\n",
        "Fake news example:\n",
        "```\n",
        "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
        "```\n",
        "\n",
        "In the dataset folder you should find 4 files, training and validation splits for both real and fake news.\n",
        "\n",
        "The project will proceed generally as follows in terms of code development:\n",
        "1. Write code to train unsmoothed unigram and bigram language models for an arbitrary corpus\n",
        "2. Implement smoothing and unknown word handling. \n",
        "3. Implement the Perplexity calculation. \n",
        "4. Using 1, 2 and 3, together with the provided training and validation sets, develop a language-model-based approach for Fake News Classification.\n",
        "5. Apply your best language-model-based news classifier (from 4) to the\n",
        "provided test set. Submit the results to the online Kaggle competition. \n",
        "6. Use any existing implementation of Naive Bayes (and the provided training and validation sets) to create an additional Naive Bayes fake news classifier. Apply your best NB classifier to the provided test set. Submit the results to the separate Kaggle competition (for NB classifiers). \n",
        "\n",
        "We will progress towards these tasks throughout this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2O1icKLpIdF"
      },
      "source": [
        "# Part 1: Preprocessing the Dataset\n",
        "In this part, you are going to do a few things:\n",
        "* Connect to the google drive where the data set is stored\n",
        "* Load and read files\n",
        "* Preprocess the text\n",
        "\n",
        "------\n",
        "**Please upload the dataset to each partner's individual Google Drive now.** We suggest using the same folder structure within Google Drive because the notebook is shared among you, so the code to load the data would have to be changed every time if folder structures are different. One folder structure might be: Google Drive/CS 4740/Project 1/Dataset/ or whatever works for you. See our code below for an example of how we load the data from Google Drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqJXHkfirHCX"
      },
      "source": [
        "## 1.1 Connect to google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "og--UAtbrP4-",
        "outputId": "ab6f5f61-f2f9-4ce0-da77-d433af57187f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yspKotLCr8kj"
      },
      "source": [
        "## 1.2 Load and read files\n",
        "First, let's install [NLTK](https://www.nltk.org/), a very widely package for NLP preprocessing (and other tasks) for Python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyUwo9CthoiW",
        "outputId": "0a984d38-b909-4d97-b24d-597a49650fe2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "!pip install -U nltk"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: nltk in /usr/local/lib/python3.6/dist-packages (3.5)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from nltk) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.6/dist-packages (from nltk) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.6/dist-packages (from nltk) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from nltk) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNdLcCH9hpYX"
      },
      "source": [
        "Then we read and load data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDIzMEQ0sMqK",
        "outputId": "800c4157-9085-453a-f544-23058c03eda2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import os\n",
        "import io\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "root_path = os.path.join(os.getcwd(), \"drive\", \"My Drive/Colab Notebooks/Project 1\") # replace based on your Google drive organization\n",
        "dataset_path = os.path.join(root_path, \"dataset\") # same here\n",
        "\n",
        "with io.open(os.path.join(dataset_path, \"trueDataTrain.txt\"), encoding='utf8') as real_file:\n",
        "  real_news = real_file.read()\n",
        "with io.open(os.path.join(dataset_path, \"trueDataValidation.txt\"), encoding='utf8') as real_file:\n",
        "  real_news_validation = real_file.read()\n",
        "with io.open(os.path.join(dataset_path, \"fakeDataTrain.txt\"), encoding='utf8') as fake_file:\n",
        "  fake_news = fake_file.read()\n",
        "with io.open(os.path.join(dataset_path, \"fakeDataValidation.txt\"), encoding='utf8') as real_file:\n",
        "  fake_news_validation = real_file.read()\n",
        "\n",
        "tokenized_real_news_training = [list(map(str.lower, word_tokenize(sent))) for sent in sent_tokenize(real_news)]\n",
        "tokenized_fake_news_training = [list(map(str.lower, word_tokenize(sent))) for sent in sent_tokenize(fake_news)]\n",
        "tokenized_real_news_validation = [list(map(str.lower, word_tokenize(sent))) for sent in sent_tokenize(real_news_validation)]\n",
        "tokenized_fake_news_validation = [list(map(str.lower, word_tokenize(sent))) for sent in sent_tokenize(fake_news_validation)]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miP2sevQbPbs"
      },
      "source": [
        "Sanity checks for our real and fake training sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BP8TzVmBj95T",
        "outputId": "8ffa36d4-45f3-4817-b455-0d9835952029",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 788
        }
      },
      "source": [
        "tokenized_real_news_training[0]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['london',\n",
              " '(',\n",
              " 'reuters',\n",
              " ')',\n",
              " '-',\n",
              " 'britain',\n",
              " 'is',\n",
              " 'seeking',\n",
              " 'to',\n",
              " 'build',\n",
              " 'on',\n",
              " 'the',\n",
              " 'recent',\n",
              " 'momentum',\n",
              " 'in',\n",
              " 'the',\n",
              " 'brexit',\n",
              " 'divorce',\n",
              " 'talks',\n",
              " 'with',\n",
              " 'the',\n",
              " 'european',\n",
              " 'union',\n",
              " 'before',\n",
              " 'a',\n",
              " 'summit',\n",
              " 'next',\n",
              " 'month',\n",
              " ',',\n",
              " 'a',\n",
              " 'spokeswoman',\n",
              " 'for',\n",
              " 'britain',\n",
              " 's',\n",
              " 'department',\n",
              " 'for',\n",
              " 'exiting',\n",
              " 'the',\n",
              " 'european',\n",
              " 'union',\n",
              " 'said',\n",
              " 'on',\n",
              " 'tuesday',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8K3DBwDmenY",
        "outputId": "18417c5e-0d3f-4f75-9689-86e73dea5748",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        }
      },
      "source": [
        "tokenized_fake_news_training[0]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['rick',\n",
              " 'santorum',\n",
              " 'had',\n",
              " 'the',\n",
              " 'impossible',\n",
              " 'job',\n",
              " 'of',\n",
              " 'defending',\n",
              " 'donald',\n",
              " 'trump',\n",
              " 'during',\n",
              " 'friday',\n",
              " 's',\n",
              " 'real',\n",
              " 'time',\n",
              " 'and',\n",
              " 'he',\n",
              " 'found',\n",
              " 'himself',\n",
              " 'humiliatingly',\n",
              " 'outnumbered.bill',\n",
              " 'maher',\n",
              " 'began',\n",
              " 'by',\n",
              " 'expressing',\n",
              " 'astonishment',\n",
              " 'at',\n",
              " 'how',\n",
              " 'the',\n",
              " 'republican',\n",
              " 'nominee',\n",
              " 'has',\n",
              " 'been',\n",
              " 'acting',\n",
              " 'throughout',\n",
              " 'the',\n",
              " 'campaign',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ok_0NzjkDrde"
      },
      "source": [
        "## 1.3 Data Preprocessing & Preparation\n",
        "\n",
        "There's a well-known parable in machine learning that 80% of the work is all about data preparation, 10% is supporting infrastructure and 10% is actual modeling. If your \"raw\" dataset is not preprocessed and prepared in a way to maximize its value, then your model will be more like this: https://xkcd.com/1838/. For this project, modeling is the star of the show for learning purposes, but we still want you to pay attention to the preprocessing stage.\n",
        "\n",
        "*We've already tokenized and lowercased* the raw data for you. Here are a few extra things you might want to do:\n",
        "\n",
        "- Think about edge cases. For example, you don't want to accidentally append a period to the last word of a sentence. \n",
        "- Watch out for apostrophes and other tricky things like quotations, they cause lots of edge cases. For example, \"they're\" can be all one token, or two tokens (\"they\", \"'re\") or even three tokens (\"they\", \" ' \", \"re\"). \n",
        "\n",
        "Why did we lowercase all tokens? Because the computer will otherwise consider \"The\" and \"the\" as two separate words and this will cause problems.\n",
        "\n",
        "Note that you may use existing\n",
        "tools just for the purpose of preprocessing. \n",
        "\n",
        "Advice: don't get bugged down in the dozens of preprocessing packages and suggestions that you can find on Towards Data Science or Stack Overflow. Start with this [NLTK tutorial](https://lost-contact.mit.edu/afs/cs.pitt.edu/projects/nltk/docs/tutorial/introduction/nochunks.html#:~:text=The%20Natural%20Language%20Toolkit%20(NLTK,tokenization%2C%20tagging%2C%20and%20parsing.) and that should be plenty."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmvEp0D5yhDc"
      },
      "source": [
        "# TODO"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEab9h6xpuhx"
      },
      "source": [
        "**Q1.1: Show some observations or statistics from the dataset** (should be quantitative â€“ i.e. most frequent words, most frequent bigram, etc.)\n",
        "\n",
        "I was curious about the distribution of word lengths in the texts, and if it was different between the real and fake news datasets. \n",
        "\n",
        "I found that more than half of the words in both datasets are between 1 and 4 (inclusive) characters. This accounted for 55% of the words in the real news training set and 57% of the words in the fake news training set.\n",
        "\n",
        "I also found that for the most part, the distributions did not significantly differ between the datasets. However, the range of word lengths is much larger in the fake news dataset. The longest word in the real news dataset was 98 characters, while the longest word in the fake news dataset was 1057 characters.\n",
        "\n",
        "The functions I used to compute these values is below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MQq0uNnqWmR"
      },
      "source": [
        "def get_word_lengths(lst):\n",
        "  length_freq = {}\n",
        "\n",
        "  for sentence in lst:\n",
        "    for word in sentence:\n",
        "      length = len(word)\n",
        "      if length in length_freq:\n",
        "        length_freq[length] += 1\n",
        "      else:\n",
        "        length_freq[length] = 1\n",
        "\n",
        "  count = sum(length_freq.values())\n",
        "  length_freq = {k: v/count for k, v in length_freq.items()}\n",
        "\n",
        "  return dict(sorted(length_freq.items()))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrOfq20hD9_k",
        "outputId": "9abee182-592a-488e-ba80-b2711778663a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 753
        }
      },
      "source": [
        "get_word_lengths(tokenized_real_news_training)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: 0.14656622098172942,\n",
              " 2: 0.13398942117196005,\n",
              " 3: 0.15205353720775422,\n",
              " 4: 0.131475069823566,\n",
              " 5: 0.10103985001610725,\n",
              " 6: 0.08408826530082052,\n",
              " 7: 0.08273580620926958,\n",
              " 8: 0.05876029779162533,\n",
              " 9: 0.04751486788079288,\n",
              " 10: 0.028899682072780333,\n",
              " 11: 0.014518380912385713,\n",
              " 12: 0.008406641200702484,\n",
              " 13: 0.004642525959573546,\n",
              " 14: 0.0030013894415887436,\n",
              " 15: 0.0011926091243461587,\n",
              " 16: 0.000444095606784974,\n",
              " 17: 0.0002984579215592065,\n",
              " 18: 0.00014915255066143656,\n",
              " 19: 7.167269084038293e-05,\n",
              " 20: 5.0736318462701776e-05,\n",
              " 21: 6.112809453337564e-05,\n",
              " 22: 1.0850236779674175e-05,\n",
              " 23: 7.641011816671954e-06,\n",
              " 24: 5.04306779900349e-06,\n",
              " 25: 3.514865435669099e-06,\n",
              " 26: 1.833842836001269e-06,\n",
              " 27: 1.0697416543340736e-06,\n",
              " 28: 2.9035844903353428e-06,\n",
              " 29: 3.667685672002538e-06,\n",
              " 30: 6.112809453337564e-07,\n",
              " 31: 1.528202363334391e-07,\n",
              " 32: 1.528202363334391e-07,\n",
              " 33: 1.528202363334391e-07,\n",
              " 38: 1.528202363334391e-07,\n",
              " 39: 3.056404726668782e-07,\n",
              " 42: 1.528202363334391e-07,\n",
              " 52: 1.528202363334391e-07,\n",
              " 59: 9.169214180006345e-07,\n",
              " 66: 3.056404726668782e-07,\n",
              " 68: 1.528202363334391e-07,\n",
              " 73: 1.528202363334391e-07,\n",
              " 98: 3.056404726668782e-07}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvM4Sh-mEDQl",
        "outputId": "b7b47fcf-35f8-4e5f-8167-68dce450c7dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "get_word_lengths(tokenized_fake_news_training)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: 0.13647876498532782,\n",
              " 2: 0.14134659141095168,\n",
              " 3: 0.1686364300815557,\n",
              " 4: 0.14521311461861375,\n",
              " 5: 0.10495256400256821,\n",
              " 6: 0.07789878185536295,\n",
              " 7: 0.07426327895424377,\n",
              " 8: 0.05097064686202367,\n",
              " 9: 0.0385029109333427,\n",
              " 10: 0.0249287569015541,\n",
              " 11: 0.013679656318231053,\n",
              " 12: 0.008533546627878327,\n",
              " 13: 0.0053886229020282785,\n",
              " 14: 0.0032338968903039655,\n",
              " 15: 0.002127091385899729,\n",
              " 16: 0.0009843866946766414,\n",
              " 17: 0.0009267930352478361,\n",
              " 18: 0.00042872410157765304,\n",
              " 19: 0.00027311791410745067,\n",
              " 20: 0.0001773006600801559,\n",
              " 21: 0.00010821409551869676,\n",
              " 22: 9.46550501374758e-05,\n",
              " 23: 9.194324106123162e-05,\n",
              " 24: 2.6085020638158407e-05,\n",
              " 25: 2.156533884441809e-05,\n",
              " 26: 0.0005818767475026821,\n",
              " 27: 2.763462582458366e-05,\n",
              " 28: 8.006293463197135e-06,\n",
              " 29: 5.294484386952944e-06,\n",
              " 30: 7.877159697661697e-06,\n",
              " 31: 4.39054802820488e-06,\n",
              " 32: 3.744879200527692e-06,\n",
              " 33: 2.0661402485670025e-06,\n",
              " 34: 1.4204714208898143e-06,\n",
              " 35: 1.756219211281952e-05,\n",
              " 36: 1.549605186425252e-06,\n",
              " 37: 4.39054802820488e-06,\n",
              " 38: 5.165350621417506e-07,\n",
              " 39: 9.039363587480636e-07,\n",
              " 40: 3.87401296606313e-07,\n",
              " 41: 2.582675310708753e-07,\n",
              " 42: 3.87401296606313e-07,\n",
              " 44: 1.2913376553543765e-07,\n",
              " 45: 9.039363587480636e-07,\n",
              " 46: 7.74802593212626e-07,\n",
              " 47: 2.4535415451733158e-06,\n",
              " 48: 1.2913376553543767e-06,\n",
              " 49: 1.549605186425252e-06,\n",
              " 50: 2.19527401410244e-06,\n",
              " 51: 1.549605186425252e-06,\n",
              " 52: 2.8409428417796287e-06,\n",
              " 53: 2.8409428417796287e-06,\n",
              " 54: 3.099210372850504e-06,\n",
              " 55: 3.486611669456817e-06,\n",
              " 56: 2.0661402485670025e-06,\n",
              " 57: 2.4535415451733158e-06,\n",
              " 58: 1.549605186425252e-06,\n",
              " 59: 3.87401296606313e-06,\n",
              " 60: 1.1622038898189389e-06,\n",
              " 61: 9.039363587480636e-07,\n",
              " 62: 3.87401296606313e-07,\n",
              " 63: 1.1622038898189389e-06,\n",
              " 64: 6.456688276771884e-07,\n",
              " 65: 2.582675310708753e-07,\n",
              " 66: 3.87401296606313e-07,\n",
              " 67: 5.165350621417506e-07,\n",
              " 68: 9.039363587480636e-07,\n",
              " 69: 5.165350621417506e-07,\n",
              " 71: 1.2913376553543765e-07,\n",
              " 72: 1.2913376553543765e-07,\n",
              " 74: 1.2913376553543765e-07,\n",
              " 75: 2.582675310708753e-07,\n",
              " 76: 1.2913376553543765e-07,\n",
              " 78: 1.2913376553543765e-07,\n",
              " 82: 2.582675310708753e-07,\n",
              " 91: 2.582675310708753e-07,\n",
              " 92: 1.2913376553543765e-07,\n",
              " 94: 2.582675310708753e-07,\n",
              " 95: 1.2913376553543765e-07,\n",
              " 96: 1.2913376553543765e-07,\n",
              " 98: 1.2913376553543765e-07,\n",
              " 99: 1.2913376553543765e-07,\n",
              " 100: 1.2913376553543765e-07,\n",
              " 102: 1.2913376553543765e-07,\n",
              " 103: 1.2913376553543765e-07,\n",
              " 104: 1.2913376553543765e-07,\n",
              " 106: 1.2913376553543765e-07,\n",
              " 112: 1.2913376553543765e-07,\n",
              " 113: 1.2913376553543765e-07,\n",
              " 120: 1.2913376553543765e-07,\n",
              " 130: 1.2913376553543765e-07,\n",
              " 139: 1.2913376553543765e-07,\n",
              " 180: 1.2913376553543765e-07,\n",
              " 917: 1.2913376553543765e-07,\n",
              " 1013: 2.582675310708753e-07,\n",
              " 1057: 2.582675310708753e-07}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcw8M5nesOQD"
      },
      "source": [
        "**Please answer the following question**:\n",
        "\n",
        "**Q1.2: What did you do in your preprocessing part?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQVi0l76MBcs"
      },
      "source": [
        "TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8oo9zHrsYBD"
      },
      "source": [
        "# Part 2: Compute Unsmoothed Language Models.\n",
        "\n",
        "To start, you will write a program that computes unsmoothed unigram and bigram probabilities. You should consider real and fake news as separate corpora and\n",
        "generate a separate language model for each set of news.\n",
        "We have already loaded the data and (partially) preprocessed it and you probably did some of your own preprocessing. \n",
        "\n",
        "Note that you were allowed to use existing\n",
        "tools for the purpose of preprocessing, but you must write the code for gathering n-gram counts and computing n-gram probabilities yourself. \n",
        "\n",
        "For example, consider the\n",
        "simple corpus consisting of the sole sentence:\n",
        "\n",
        "\n",
        "> the students liked the assignment\n",
        "\n",
        "Part of what your program would compute for a unigram and bigram model, for example,\n",
        "would be the following:\n",
        "\n",
        "\n",
        "> $P(\"the\") = 0.4; P(\"liked\") = 0.2; P(\"the\"|\"liked\") = 1.0; P(\"students\"|\"the\") = 0.5$\n",
        "\n",
        "Remember to add a symbol to mark the beginning of sentence. See Sept. 7th lecture, p25-28 for an example.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5y3VQVQw_TZ"
      },
      "source": [
        "print_text=tokenized_fake_news_training[3]"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mticrzslHhe8"
      },
      "source": [
        "# Returns a dictionary that maps a unigram : str to its number of occurrences\n",
        "# Requires `text` is a list of tokens : str\n",
        "def unigram_counts(text):\n",
        "  counts = {}\n",
        "\n",
        "  for t in text:\n",
        "    counts[t] = 1 if t not in counts else counts[t] + 1\n",
        "\n",
        "  return counts"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BO0jmSBrIHJt"
      },
      "source": [
        "# Returns a dictionary that maps a bigram : (str, str) to its number of occurrences\n",
        "# Requires `text` is a list of tokens : str\n",
        "def bigram_counts(text):\n",
        "  words = set(text)\n",
        "  counts = {}       ## we don't store counts that are 0\n",
        "\n",
        "  for i in range(1, len(text)):\n",
        "    b = (text[i-1], text[i])\n",
        "    counts[b] = 1 if b not in counts else counts[b] + 1\n",
        "\n",
        "  return counts"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jG-ea1JLAn9",
        "outputId": "d506f6bb-77e9-4715-9157-02c1a804e856",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        }
      },
      "source": [
        "unigram_counts(print_text)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'.': 1,\n",
              " 'he': 1,\n",
              " 'it': 1,\n",
              " 'like': 1,\n",
              " 'lose': 1,\n",
              " 'not': 1,\n",
              " 's': 2,\n",
              " 'to': 1,\n",
              " 'trying': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hT6mYJYQphh",
        "outputId": "c851b017-b04e-4d71-c7af-230e279d1a4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        }
      },
      "source": [
        "bigram_counts(print_text)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{('he', 's'): 1,\n",
              " ('it', 's'): 1,\n",
              " ('like', 'he'): 1,\n",
              " ('lose', '.'): 1,\n",
              " ('not', 'like'): 1,\n",
              " ('s', 'not'): 1,\n",
              " ('s', 'trying'): 1,\n",
              " ('to', 'lose'): 1,\n",
              " ('trying', 'to'): 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqkMkZIkst8i"
      },
      "source": [
        "## 2.1 Unsmoothed Uni-gram Model\n",
        "\n",
        "> Indented block\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSpCXMDUs3Xs"
      },
      "source": [
        "# Returns a dictionary that maps a unigram : str to its (prior) probability\n",
        "# Requires `text` is a list of tokens : str\n",
        "def unigram_model(text):\n",
        "  counts_dict = unigram_counts(text) # dictionary maps tokens to counts\n",
        "  total_count = sum(counts_dict.values())\n",
        "  probabilities = {t : c/total_count for t, c in counts_dict.items()}\n",
        "  return probabilities"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "km82WWLKKwcy",
        "outputId": "b54254c5-d563-4bdd-add6-daa358dc9755",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        }
      },
      "source": [
        "unigram_model(print_text)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'.': 0.1,\n",
              " 'he': 0.1,\n",
              " 'it': 0.1,\n",
              " 'like': 0.1,\n",
              " 'lose': 0.1,\n",
              " 'not': 0.1,\n",
              " 's': 0.2,\n",
              " 'to': 0.1,\n",
              " 'trying': 0.1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuyn_xPjs4TR"
      },
      "source": [
        "## 2.2 Unsmoothed Bi-gram Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7opVGtks_kY"
      },
      "source": [
        "# Returns a dictionary that maps a bigram : (str, str) to its (prior) probability\n",
        "# Requires `text` is a list of tokens : str\n",
        "def bigram_model(text):\n",
        "  # dictionaries mapping n-grams to counts\n",
        "  unigram_count_dict = unigram_counts(text)\n",
        "  bigram_counts_dict = bigram_counts(text)\n",
        "\n",
        "  probabilities = { (t1, t2) : c/unigram_count_dict[t1] for (t1, t2), c in bigram_counts_dict.items() }\n",
        "  return probabilities"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mr3g42WoKtGN",
        "outputId": "e7ef39b5-797d-4e9e-90b4-cd83240490cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        }
      },
      "source": [
        "bigram_model(print_text)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{('he', 's'): 1.0,\n",
              " ('it', 's'): 1.0,\n",
              " ('like', 'he'): 1.0,\n",
              " ('lose', '.'): 1.0,\n",
              " ('not', 'like'): 1.0,\n",
              " ('s', 'not'): 0.5,\n",
              " ('s', 'trying'): 0.5,\n",
              " ('to', 'lose'): 1.0,\n",
              " ('trying', 'to'): 1.0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejmIJWSVwRJG"
      },
      "source": [
        "**Q2: What data structure are you using to store probabilities for unigrams and bigrams? Why did you select this data structure?**\n",
        "\n",
        "I used a dictionary (i.e. hash table) because we will need to index the count and probability of a given unigram or bigram, and dictionaries have a constant-time indexing operation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miWWgYxZtAZu"
      },
      "source": [
        "# Part 3: Smoothed Language Model\n",
        "In this part, you will need to implement **at least one** smoothing method and **at least one** method to handle unknown words in the test data. You can choose any method(s) that you want for each. You should make clear\n",
        "**what method(s)** were selected and **why**, providing a description for any non-standard approach (e.g., an approach that was not covered in class or in the readings). \n",
        "\n",
        "You should use the\n",
        "provided validation sets to experiment with different smoothing/unknown word handling\n",
        "methods if you wish to see which one is more effective for this task. (We will cover this in Part 4)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfiAOKfEuMGy"
      },
      "source": [
        "## 3.1 Unknown Words Handling\n",
        "\n",
        "**Q3.1: How are you going to handle unknown words? What parameters might be needed? Do you need a method to determine the value?**\n",
        "\n",
        "We first sort the tokens by their frequency. We then take the bottom `alpha`-percentile of tokens and replace them with the symbol for unknown words `UNK`. This `UNK` symbol is treated as any other token in the n-gram probability computations.\n",
        "\n",
        "This is equivalent to implicitly creating a closed vocabulary of the `(1-alpha)` percent most frequent tokens. \n",
        "\n",
        "We will need to choose `alpha` using a validation set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92KxlweHeGq1"
      },
      "source": [
        "UNK = '<UNK>' # marks an unknown word"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xFOxRuJfVLU"
      },
      "source": [
        "# Returns the text with the bottom `alpha`-percentile of tokens by frequency\n",
        "#   replaced with the unknown word symbol UNK\n",
        "# Requires `text` is a list of tokens : str\n",
        "def generate_unk_text(text, alpha):\n",
        "  # compute frequency of each token\n",
        "  token_freq = unigram_counts(text)\n",
        "  # sort tokens in order of ascending frequency\n",
        "  sorted_tokens = sorted(token_freq, key=token_freq.get)\n",
        "  # get the bottom `alpha`-percentile of tokens by frequency\n",
        "  alpha_tokens = set(sorted_tokens[:int(len(sorted_tokens)*alpha)])\n",
        "  # map original text with UNK-replacement funcion\n",
        "  unk_text = list(map(lambda t: UNK if t in alpha_tokens else t, text))\n",
        "  return unk_text"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMnVu28skAcE",
        "outputId": "e7ca686d-bfec-4c50-e836-89a6f88e49d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "generate_unk_text(print_text, 0.3)"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<UNK>', 's', '<UNK>', 'like', 'he', 's', 'trying', 'to', 'lose', '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRGK0_x_t46P"
      },
      "source": [
        "## 3.2 Smoothing\n",
        "\n",
        "In this part of project, we are going to compute the probabilities for unigram and bigram models after smoothing.\n",
        "There are several smoothing methods you can start with:\n",
        "* add-k\n",
        "* Kneser-Ney\n",
        "* Good-Turing\n",
        "* ...\n",
        "\n",
        "You need to compute for both unigram and bigram models.\n",
        "\n",
        "Below is a starter point using add-k smoothing. As always, you DO NOT need to follow it; you DO NOT need to use add-k smoothing if you do not want to. You can pick ANY smoothing method you want."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmW8G3mqt-5z"
      },
      "source": [
        "\"\"\"\n",
        "Function [add_k_unigram] applies add-k smoothing to a unigram model\n",
        "dic: a dictionary of your unigrams. key: words, val: occurence\n",
        "k: parameter k for smoothing\n",
        "Return: a dictionary of results after smoothing\n",
        "\"\"\"\n",
        "def add_k_unigram(dic, k):\n",
        "  total = sum(dic.values())\n",
        "  unique = len(dic)\n",
        "  return {w: (v+k)/(total + k*unique) for w, v in dic.items()}\n",
        "\n",
        "def "
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-qlEsrxNQXP",
        "outputId": "7e2c335d-f43f-4283-87e3-6b97d5fe258a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        }
      },
      "source": [
        "lst=tokenized_fake_news_training[3]\n",
        "dic=unigram_counts(lst)\n",
        "add_k_unigram(dic, 1)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'.': 0.10526315789473684,\n",
              " 'he': 0.10526315789473684,\n",
              " 'it': 0.10526315789473684,\n",
              " 'like': 0.10526315789473684,\n",
              " 'lose': 0.10526315789473684,\n",
              " 'not': 0.10526315789473684,\n",
              " 's': 0.15789473684210525,\n",
              " 'to': 0.10526315789473684,\n",
              " 'trying': 0.10526315789473684}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Szs3zqwSFDEY"
      },
      "source": [
        "\"\"\"\n",
        "Function [add_k_bigram] applies add-k smoothing to a bigram model\n",
        "uni_dic: a dictionary of your unigrams.\n",
        "bi_dic: a dictionary of your bigrams.\n",
        "k: parameter k for smoothing\n",
        "Return: a function that takes a bigram and returns a probability\n",
        "\"\"\"\n",
        "def add_k_bigram(uni_dic, bi_dic, k):\n",
        "  unique = len(uni_dic)\n",
        "  return {(w0, w1): (v+k)/(uni_dic[w0] + k*unique) for (w0, w1), v in bi_dic.items()}\n",
        "\n",
        "# Returns k-smoothed probability of `bigram` given MLE counts\n",
        "# Note that `bigram` is a pair (w0, w1) of tokens \n",
        "def get_k_bigram(uni_counts, bi_counts, k, bigram):\n",
        "  unsmoothed = bi_counts[bigram]\n",
        "  \n",
        "\n"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqJVmo6BOiRL",
        "outputId": "cc0dde96-4296-4089-c8c9-0dd05d0098b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "lst=tokenized_fake_news_training[3]\n",
        "uni_dic=unigram_counts(lst)\n",
        "bi_dic=bigram_counts(lst)\n",
        "add_k_bigram(uni_dic, bi_dic, 0.5)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{('.', '.'): 0.09090909090909091,\n",
              " ('.', 'he'): 0.09090909090909091,\n",
              " ('.', 'it'): 0.09090909090909091,\n",
              " ('.', 'like'): 0.09090909090909091,\n",
              " ('.', 'lose'): 0.09090909090909091,\n",
              " ('.', 'not'): 0.09090909090909091,\n",
              " ('.', 's'): 0.09090909090909091,\n",
              " ('.', 'to'): 0.09090909090909091,\n",
              " ('.', 'trying'): 0.09090909090909091,\n",
              " ('he', '.'): 0.09090909090909091,\n",
              " ('he', 'he'): 0.09090909090909091,\n",
              " ('he', 'it'): 0.09090909090909091,\n",
              " ('he', 'like'): 0.09090909090909091,\n",
              " ('he', 'lose'): 0.09090909090909091,\n",
              " ('he', 'not'): 0.09090909090909091,\n",
              " ('he', 's'): 0.2727272727272727,\n",
              " ('he', 'to'): 0.09090909090909091,\n",
              " ('he', 'trying'): 0.09090909090909091,\n",
              " ('it', '.'): 0.09090909090909091,\n",
              " ('it', 'he'): 0.09090909090909091,\n",
              " ('it', 'it'): 0.09090909090909091,\n",
              " ('it', 'like'): 0.09090909090909091,\n",
              " ('it', 'lose'): 0.09090909090909091,\n",
              " ('it', 'not'): 0.09090909090909091,\n",
              " ('it', 's'): 0.2727272727272727,\n",
              " ('it', 'to'): 0.09090909090909091,\n",
              " ('it', 'trying'): 0.09090909090909091,\n",
              " ('like', '.'): 0.09090909090909091,\n",
              " ('like', 'he'): 0.2727272727272727,\n",
              " ('like', 'it'): 0.09090909090909091,\n",
              " ('like', 'like'): 0.09090909090909091,\n",
              " ('like', 'lose'): 0.09090909090909091,\n",
              " ('like', 'not'): 0.09090909090909091,\n",
              " ('like', 's'): 0.09090909090909091,\n",
              " ('like', 'to'): 0.09090909090909091,\n",
              " ('like', 'trying'): 0.09090909090909091,\n",
              " ('lose', '.'): 0.2727272727272727,\n",
              " ('lose', 'he'): 0.09090909090909091,\n",
              " ('lose', 'it'): 0.09090909090909091,\n",
              " ('lose', 'like'): 0.09090909090909091,\n",
              " ('lose', 'lose'): 0.09090909090909091,\n",
              " ('lose', 'not'): 0.09090909090909091,\n",
              " ('lose', 's'): 0.09090909090909091,\n",
              " ('lose', 'to'): 0.09090909090909091,\n",
              " ('lose', 'trying'): 0.09090909090909091,\n",
              " ('not', '.'): 0.09090909090909091,\n",
              " ('not', 'he'): 0.09090909090909091,\n",
              " ('not', 'it'): 0.09090909090909091,\n",
              " ('not', 'like'): 0.2727272727272727,\n",
              " ('not', 'lose'): 0.09090909090909091,\n",
              " ('not', 'not'): 0.09090909090909091,\n",
              " ('not', 's'): 0.09090909090909091,\n",
              " ('not', 'to'): 0.09090909090909091,\n",
              " ('not', 'trying'): 0.09090909090909091,\n",
              " ('s', '.'): 0.07692307692307693,\n",
              " ('s', 'he'): 0.07692307692307693,\n",
              " ('s', 'it'): 0.07692307692307693,\n",
              " ('s', 'like'): 0.07692307692307693,\n",
              " ('s', 'lose'): 0.07692307692307693,\n",
              " ('s', 'not'): 0.23076923076923078,\n",
              " ('s', 's'): 0.07692307692307693,\n",
              " ('s', 'to'): 0.07692307692307693,\n",
              " ('s', 'trying'): 0.23076923076923078,\n",
              " ('to', '.'): 0.09090909090909091,\n",
              " ('to', 'he'): 0.09090909090909091,\n",
              " ('to', 'it'): 0.09090909090909091,\n",
              " ('to', 'like'): 0.09090909090909091,\n",
              " ('to', 'lose'): 0.2727272727272727,\n",
              " ('to', 'not'): 0.09090909090909091,\n",
              " ('to', 's'): 0.09090909090909091,\n",
              " ('to', 'to'): 0.09090909090909091,\n",
              " ('to', 'trying'): 0.09090909090909091,\n",
              " ('trying', '.'): 0.09090909090909091,\n",
              " ('trying', 'he'): 0.09090909090909091,\n",
              " ('trying', 'it'): 0.09090909090909091,\n",
              " ('trying', 'like'): 0.09090909090909091,\n",
              " ('trying', 'lose'): 0.09090909090909091,\n",
              " ('trying', 'not'): 0.09090909090909091,\n",
              " ('trying', 's'): 0.09090909090909091,\n",
              " ('trying', 'to'): 0.2727272727272727,\n",
              " ('trying', 'trying'): 0.09090909090909091}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbHkdI7-t_7D"
      },
      "source": [
        "**Please answer the following question:**\n",
        "\n",
        "**Q3.2: Which smoothing method did you choose? Are there any parameters, if so how are you planning to pick the value? If you choose to implement more than 1 method (not a requirement), please state each of them. Providing a description for any non-standard approach, e.g., an approach that was not covered in class or in the readings**\n",
        "\n",
        "I chose to use add-k smoothing, which only has a single parameter, that being k. I will pick the value of k by optimizing on a validation set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ML03appeuSm3"
      },
      "source": [
        "# Part 4: Perplexity\n",
        "At this point, we have developed several language models: unigram vs bigram, unsmoothed vs smoothed. We now want to compare all the models. \n",
        "\n",
        "Implement code to compute the perplexity of a **â€œdevelopment set.â€** (â€œDevelopment setâ€\n",
        "is just another way to refer to the validation setâ€”part of a dataset that is distinct from\n",
        "the training portion and the test portion.) Compute and report the perplexity of each\n",
        "of the language models (one trained on true news and fake news) on\n",
        "the development corpora. Compute perplexity as follows:\n",
        "\\begin{align*}\n",
        "PP &= \\left(\\prod_i^N\\frac{1}{P\\left(W_i\\mid W_{i-1}, ...W_{i-n+1}\\right)}\\right)^{\\frac{1}{N}}\\\\\n",
        "&=\\exp \\frac{1}{N}\\sum_{i}^N-\\log P\\left(W_i\\mid W_{i-1}, ...W_{i-n+1}\\right)\n",
        "\\end{align*}\n",
        "where $N$ is the total number of tokens in the test corpus and $P\\left(W_i\\mid W_{i-1}, ...W_{i-n+1}\\right)$\n",
        "is the n-gram probability of your model. Under the second definition above, perplexity\n",
        "is a function of the average (per-word) log probability: use this to avoid numerical\n",
        "computation errors.\n",
        "\n",
        "Please complete the following tasks and report what you have observed. Remember, lower perplexity means better model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aT1IRyohJw7e"
      },
      "source": [
        "## Task 1: Compute perplexity for smoothed unigram and smoothed bigram. \n",
        "*Note: If you choose more than one smoothing method, pick one of them to compute. If you need to try different values of parameters, you can try them out here.*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOhb-Ws1yqWi"
      },
      "source": [
        "START = '<START>' # marks the beginning of a sentence\n",
        "END   = '<END>'   # marks the end of a sentence"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pf2phKs8bvaB"
      },
      "source": [
        "\"\"\"\n",
        "Function [tokenize_text] processes a list of tokenized sentences\n",
        "text: a list of sentences which are tokenized\n",
        "Return: a list of tokens with special tokens inserted where appropriate\n",
        "\"\"\"\n",
        "def tokenize_text(text):\n",
        "  new_text = []\n",
        "  for i in range(len(text)):\n",
        "    new_text += [START] + text[i] + [END]\n",
        "  return new_text"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0LyD8mXyTyr"
      },
      "source": [
        "processed_real_news_training = tokenize_text(tokenized_real_news_training)\n",
        "processed_fake_news_training = tokenize_text(tokenized_fake_news_training)\n",
        "processed_real_news_validation = tokenize_text(tokenized_real_news_validation)\n",
        "processed_fake_news_validation = tokenize_text(tokenized_fake_news_validation)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFFj7XqDKXfX"
      },
      "source": [
        "import math\n",
        "\n",
        "\"\"\"\n",
        "train: the (processed) training set\n",
        "dev: the (processed) development set\n",
        "alpha: the percentage of the most infrequent word types to replace with UNK\n",
        "k: the value of k for add-k smoothing\n",
        "Return: the perplexity of a unigram model on the development set\n",
        "\"\"\"\n",
        "def smooth_unigram_perplexity(train, dev, alpha=0.1, k=0.5):\n",
        "  unigram = train_unigram(train, alpha, k)\n",
        "\n",
        "  sum_term = 0\n",
        "  for w in dev:\n",
        "    try:\n",
        "      probability = -1 * math.log(train_unigram[w])\n",
        "    except KeyError:\n",
        "      probability = -1 * math.log(train_unigram[UNK])\n",
        "    sum_term += probability\n",
        "\n",
        "  return math.exp(sum_term / len(dev))"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAvTAHK66MkD",
        "outputId": "f3374d88-42b0-4f31-9778-35adf281d6a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "real_pp_unigram = smooth_unigram_perplexity(processed_real_news_training, processed_real_news_validation)\n",
        "fake_pp_unigram = smooth_unigram_perplexity(processed_fake_news_training, processed_fake_news_validation)\n",
        "\n",
        "print(\"Unigram PP\")\n",
        "print(\"Real news model:\", real_pp_unigram)\n",
        "print(\"Fake news model:\", fake_pp_unigram)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unigram PP\n",
            "Real news model: 873.0566349642545\n",
            "Fake news model: 1026.0685770305938\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bs3u3Xwp49wL"
      },
      "source": [
        "import math\n",
        "\n",
        "\"\"\"\n",
        "train: the (processed) training set\n",
        "dev: the (processed) development set\n",
        "alpha: the percentage of the most infrequent word types to replace with UNK\n",
        "k: the value of k for add-k smoothing\n",
        "Return: the perplexity of a unigram model on the development set\n",
        "\"\"\"\n",
        "def smooth_bigram_perplexity(train, dev, alpha=0.1, k=0.5):\n",
        "  bigram = train_bigram(train, alpha, k)\n",
        "\n",
        "  sum_term = 0\n",
        "  for i in range(1, len(dev)):\n",
        "    w0, w1 = text[i-1], text[i]\n",
        "    b = (w0, w1)\n",
        "    \n",
        "    try:\n",
        "      probability = -1 * math.log(train_bigram[b])\n",
        "    except KeyError:\n",
        "      probability = -1 * math.log(train_bigram[(w0, UNK)])\n",
        "    sum_term += probability\n",
        "\n",
        "  return math.exp(sum_term / len(dev))"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8ITVKmy7hJV",
        "outputId": "a040f9ea-b091-4dfd-f8d0-074c6cd24ae1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        }
      },
      "source": [
        "real_pp_bigram = smooth_bigram_perplexity(processed_real_news_training, processed_real_news_validation)\n",
        "fake_pp_bigram = smooth_bigram_perplexity(processed_fake_news_training, processed_fake_news_validation)\n",
        "\n",
        "print(\"Bigram PP\")\n",
        "print(\"Real news model:\", real_pp_bigram)\n",
        "print(\"Fake news model:\", fake_pp_bigram)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-ab862ed1c8b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreal_pp_bigram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmooth_bigram_perplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_real_news_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessed_real_news_validation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfake_pp_bigram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmooth_bigram_perplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_fake_news_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessed_fake_news_validation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Bigram PP\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Real news model:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_pp_bigram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-75-635165c949c4>\u001b[0m in \u001b[0;36msmooth_bigram_perplexity\u001b[0;34m(train, dev, alpha, k)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msmooth_bigram_perplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mtrain_unk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_unk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0mtrain_unigram_counts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munigram_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_unk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m   \u001b[0mtrain_bigram_counts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbigram_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_unk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0mtrain_bigram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_k_bigram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_unigram_counts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_bigram_counts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-5c03c7d8cb05>\u001b[0m in \u001b[0;36munigram_counts\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mcounts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcounts\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbxTiHHh7Lwc"
      },
      "source": [
        "**Q4.1: Why do we need to compute perplexity after smoothing?**\n",
        "\n",
        "N-gram models are subject to data sparsity because most corpa arenâ€™t\n",
        "large enough to contain all possible N-grams.\n",
        "\n",
        "Any N-gram that is not in the training set is computed to have a probability of 0.\n",
        "\n",
        "If the probability of any word in the test set is computed to be 0, the probability of the entire test set is also computed to be 0. Since we cannot take the inverse, we cannot compute perplexity, and therefore cannot evaluate the model at all.\n",
        "\n",
        "Therefore we need to introduce smoothing before computing perplexity.\n",
        "\n",
        "**Q4.2: Did you choose any values for parameters?**\n",
        "\n",
        "I had to choose values for alpha and k. I chose alpha to be 0.1 since there wasn't much improvement above that value, and intuitively I don't want to remove to many infrequent words from the dataset. I chose k to be 0.5 although this value seemed to affect the total perplexity much less."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQxfllB0KfKf"
      },
      "source": [
        "## Task 2: Compute perplexity for other smoothing methods (optional). \n",
        "*Note: If you only pick one smoothing method, you can omit this task. If you need to try different values of parameters, you can try them out here.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fU-sCOYK_xI"
      },
      "source": [
        "# TODO: compute perplexity for your rest of smoothing method."
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaALsOnk7FZJ"
      },
      "source": [
        "**Q4.3: If your smoothing method needs to pick a parameter, what is the value of your parameter?**\n",
        "\n",
        "Your answer:\n",
        "\n",
        "**Q4.4: Which smoothing method is the best among your choices?**\n",
        "\n",
        "Your answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKH7JpZWvdP5"
      },
      "source": [
        "# Part 5: Putting Everything Together and Submitting to Kaggle\n",
        "Combining all the previous parts together, we have developed a bunch of language models. Before we proceed to the next step, let's check a few things (no need to answer):\n",
        "* Did you train your model only on training set?\n",
        "* Did you validate your model only on validation/development set?\n",
        "* Did you determine all your parameters?\n",
        "\n",
        "Finally, please answer:\n",
        "\n",
        "**Q5: What is your choice of language model, and why?**\n",
        "\n",
        "I chose to use the smoothed bigram language model since that had the lowest PP.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukEVlpFow6mz"
      },
      "source": [
        "# TODO: anything that helps you answer/check the above points."
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7bWKZQjv2Ht"
      },
      "source": [
        "## Part 5.1: First Model Submission to Kaggle\n",
        "\n",
        "Now we need to apply our model to testing data. What you need to do:\n",
        "* Takes the test data as input, and generates an output of your prediction based on your chosen language model\n",
        "* Your output file should be ONLY your predictions\n",
        "* Submit to Kaggle\n",
        "\n",
        "You should use your trained model to predict labels for all the news in `TestData.txt`. Output your predictions to a **csv** file and submit it to kaggle. Each line should contain the id of the test news and its corresponding prediction (in total 4489 lines). In other words, your output should look like (**including the header**):\n",
        "```\n",
        "Id,Prediction\n",
        "0,0\n",
        "1,0\n",
        "2,1\n",
        "3,0\n",
        "...\n",
        "4488,1\n",
        "```\n",
        "Note that you should add the header `Id,Prediction` and there is no space in the output. The Id starts from 0 (not 1).\n",
        "\n",
        "Use this kaggle [link](https://www.kaggle.com/t/a032d54898b24b4bb2f29e7165ac5fdf) to submit your output. Your team name should be the concatenation of your netids, **exactly in the same order as this notebook is named**. For example, if notebook is 4740_FA20_p1_jb123_cj456, then Kaggle group should be jb123_cj456.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqECbQs7wDFr"
      },
      "source": [
        "# TODO: Add code to generate the Kaggle output file and submit the output file to Kaggle"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBtISMsewD4e"
      },
      "source": [
        "# Part 6: Naive Bayes\n",
        "\n",
        "The Naive Bayes classification method is based on Bayes Rule. Suppose we have a news article *d* and its label *c* (either 0 or 1).\n",
        "\\begin{align*}\n",
        "P(c|d)=\\frac{P(d|c)P(c)}{P(d)}\n",
        "\\end{align*}\n",
        "Likelihood: $P(d|c)$. In real/fake corpus, how likely *d* would appear.\n",
        "\n",
        "Prior: $P(c)$. The probability of real/fake news in general.\n",
        "\n",
        "Posterior: $P(c|d)$. Given *d*, how likely is it that it is real/fake.\n",
        "\n",
        "Goal: $\\underset{c\\in \\{0,1\\}}{\\operatorname{argmax}} P(c|d)$, which is equivalent to $\\underset{c\\in \\{0,1\\}}{\\operatorname{argmax}} P(d|c)P(c)$.\n",
        "\n",
        "The equivalence holds because $P(d)$ is the same for any $c$. Thus the denominator can be dropped.\n",
        "\n",
        "Denote $d=\\{x_1, x_2, ..., x_n\\}$ where $x_i$'s are words in the news *d* (sometimes called features). Unlike n-gram language modelling, we make the multinomial Naive Bayes independence assumption here, where we assume positions of words do not matter. Formally, \n",
        "\\begin{align*}\n",
        "&\\underset{c\\in \\{0,1\\}}{\\operatorname{argmax}} P(d|c)P(c)\\\\\n",
        "=&\\underset{c\\in \\{0,1\\}}{\\operatorname{argmax}} P(x_1, ..., x_n|c)P(c)\\\\\n",
        "=&\\underset{c\\in \\{0,1\\}}{\\operatorname{argmax}} P(x_1|c)P(x_2|c)...P(x_n|c)\n",
        "\\end{align*}\n",
        "\n",
        "Now we only need to collect the occurences of each word for the classification. This is often called a **bag of words** feature. \n",
        "\n",
        "For instance, in the sentence `All for one and one for all .`, the bag of words feature would be `{\"all\": 2, \"for\": 2, \"one\": 2, \"and\": 1, \".\": 1}`. Essentially, the bag of words feature is a dictionary which maps the word to its occurences. We can see that the order is not considered here.\n",
        "\n",
        "Now, your goal is to implement the Multinomial Naive Bayes. You can use existing codes or Python packages, and adapt them to our news classification task.\n",
        "\n",
        "You might find the following packages/functions useful:\n",
        "\n",
        "* nltk.word_tokenize(), nltk.word_tokenize()\n",
        "* nltk.classify.naivebayes()\n",
        "* sklearn.feature_extraction.text\n",
        "* sklearn.naive_bayes.MultinomialNB()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtQpd3ciodGZ"
      },
      "source": [
        "**Please answer the following question(s).**\n",
        "\n",
        "**Q6: Comparing Multinomial Naive Bayes with the unigram language model, which one do you expect to perform better? Why?**\n",
        "\n",
        "A: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaZTryrewV4I"
      },
      "source": [
        "## 6.1 Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmaInFPSwY9G"
      },
      "source": [
        "# TODO: Naive Bayes implementation "
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWurBZeiwZmT"
      },
      "source": [
        "## 6.2 Putting Everything Together and Submitting to Kaggle\n",
        "\n",
        "You should use your trained model to predict labels for all the news in `TestData.txt`. Output your predictions to a **csv** file and submit it to kaggle. The format should follow Part 6 as well.\n",
        "\n",
        "Use this Kaggle [link](https://www.kaggle.com/t/e8e4f7ee507843d1902c168529d705c8) to submit your output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3pTjTh6wjI-"
      },
      "source": [
        "# TODO: Code for predicting the test labels and generating the output file. Then submit the output file to Kaggle"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MVtIIyEzSS2"
      },
      "source": [
        "# Work Distribution\n",
        "\n",
        "**Please briefly describe how you divided the work.**\n",
        "\n",
        "Your answer:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddOznZ7P2nlI"
      },
      "source": [
        "# Project Feedback [1 point]\n",
        "It goes without saying that this is an unprecedented time. We on the course staff are trying our best to adapt our teaching, projects and everything else in the class to this new modality. We would immenselly appreciate it if you could provide us feedback (it's a super short form!!) on this project and **it's worth 1 point of your project grade**\n",
        "\n",
        "Link to the feedback form: https://forms.gle/xfonP9rJk9Uk7zaSA \n",
        "\n",
        "We will use this feedback to improve both **upcoming projects** and projects for next year. \n",
        "\n",
        "Thank you so much!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBrWSR2bs77m"
      },
      "source": [
        "# Submitting the Notebook\n",
        "\n",
        "1. Go to File (upper left corner) -> Download .ipynb -> submit this downloaded file to cms\n",
        "2. Run the first code block\n",
        "3. Replace our placeholder for your correct Google Drive directory structure in the 2nd code block below. Run the code block\n",
        "4. Put the name of this notebook into our placeholder in the 3rd code block. Run the code block\n",
        "5. Then go to the folder icon on the very left panel, under the orange CO logo. Click on the folder and wait for a PDF version of your notebook to appear. Might take a few minutes.\n",
        "6. Download the pdf version and submit to Gradescope"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jg5v1sRzP5m"
      },
      "source": [
        "%%capture\n",
        "!apt-get install texlive texlive-xetex texlive-latex-extra pandoc\n",
        "!pip install pypandoc"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eOenvlzzRTh"
      },
      "source": [
        "%%capture\n",
        "# the red text is a placeholder! Change it to your directory structure!\n",
        "!cp 'drive/My Drive/Colab Notebooks/4740_FA20_p1_aak85_jb123.ipynb' ./ "
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOMiIp6Uzj1S",
        "outputId": "a6f25407-b3f8-41fe-b815-aedbd99515c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# the red text is a placeholder! Change it to the name of this notebook!\n",
        "!jupyter nbconvert --to PDF \"4740_FA20_p1_aak85_jb123.ipynb\""
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[NbConvertApp] WARNING | pattern u'4740_FA20_p1_aak85_jb123.ipynb' matched no files\n",
            "This application is used to convert notebook files (*.ipynb) to various other\n",
            "formats.\n",
            "\n",
            "WARNING: THE COMMANDLINE INTERFACE MAY CHANGE IN FUTURE RELEASES.\n",
            "\n",
            "Options\n",
            "-------\n",
            "\n",
            "Arguments that take values are actually convenience aliases to full\n",
            "Configurables, whose aliases are listed on the help line. For more information\n",
            "on full configurables, see '--help-all'.\n",
            "\n",
            "--execute\n",
            "    Execute the notebook prior to export.\n",
            "--allow-errors\n",
            "    Continue notebook execution even if one of the cells throws an error and include the error message in the cell output (the default behaviour is to abort conversion). This flag is only relevant if '--execute' was specified, too.\n",
            "--no-input\n",
            "    Exclude input cells and output prompts from converted document. \n",
            "    This mode is ideal for generating code-free reports.\n",
            "--stdout\n",
            "    Write notebook output to stdout instead of files.\n",
            "--stdin\n",
            "    read a single notebook file from stdin. Write the resulting notebook with default basename 'notebook.*'\n",
            "--inplace\n",
            "    Run nbconvert in place, overwriting the existing notebook (only \n",
            "    relevant when converting to notebook format)\n",
            "-y\n",
            "    Answer yes to any questions instead of prompting.\n",
            "--clear-output\n",
            "    Clear output of current file and save in place, \n",
            "    overwriting the existing notebook.\n",
            "--debug\n",
            "    set log level to logging.DEBUG (maximize logging output)\n",
            "--no-prompt\n",
            "    Exclude input and output prompts from converted document.\n",
            "--generate-config\n",
            "    generate default config file\n",
            "--nbformat=<Enum> (NotebookExporter.nbformat_version)\n",
            "    Default: 4\n",
            "    Choices: [1, 2, 3, 4]\n",
            "    The nbformat version to write. Use this to downgrade notebooks.\n",
            "--output-dir=<Unicode> (FilesWriter.build_directory)\n",
            "    Default: ''\n",
            "    Directory to write output(s) to. Defaults to output to the directory of each\n",
            "    notebook. To recover previous default behaviour (outputting to the current\n",
            "    working directory) use . as the flag value.\n",
            "--writer=<DottedObjectName> (NbConvertApp.writer_class)\n",
            "    Default: 'FilesWriter'\n",
            "    Writer class used to write the  results of the conversion\n",
            "--log-level=<Enum> (Application.log_level)\n",
            "    Default: 30\n",
            "    Choices: (0, 10, 20, 30, 40, 50, 'DEBUG', 'INFO', 'WARN', 'ERROR', 'CRITICAL')\n",
            "    Set the log level by value or name.\n",
            "--reveal-prefix=<Unicode> (SlidesExporter.reveal_url_prefix)\n",
            "    Default: u''\n",
            "    The URL prefix for reveal.js (version 3.x). This defaults to the reveal CDN,\n",
            "    but can be any url pointing to a copy  of reveal.js.\n",
            "    For speaker notes to work, this must be a relative path to a local  copy of\n",
            "    reveal.js: e.g., \"reveal.js\".\n",
            "    If a relative path is given, it must be a subdirectory of the current\n",
            "    directory (from which the server is run).\n",
            "    See the usage documentation\n",
            "    (https://nbconvert.readthedocs.io/en/latest/usage.html#reveal-js-html-\n",
            "    slideshow) for more details.\n",
            "--to=<Unicode> (NbConvertApp.export_format)\n",
            "    Default: 'html'\n",
            "    The export format to be used, either one of the built-in formats\n",
            "    ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf',\n",
            "    'python', 'rst', 'script', 'slides'] or a dotted object name that represents\n",
            "    the import path for an `Exporter` class\n",
            "--template=<Unicode> (TemplateExporter.template_file)\n",
            "    Default: u''\n",
            "    Name of the template file to use\n",
            "--output=<Unicode> (NbConvertApp.output_base)\n",
            "    Default: ''\n",
            "    overwrite base name use for output files. can only be used when converting\n",
            "    one notebook at a time.\n",
            "--post=<DottedOrNone> (NbConvertApp.postprocessor_class)\n",
            "    Default: u''\n",
            "    PostProcessor class used to write the results of the conversion\n",
            "--config=<Unicode> (JupyterApp.config_file)\n",
            "    Default: u''\n",
            "    Full path of a config file.\n",
            "\n",
            "To see all available configurables, use `--help-all`\n",
            "\n",
            "Examples\n",
            "--------\n",
            "\n",
            "    The simplest way to use nbconvert is\n",
            "    \n",
            "    > jupyter nbconvert mynotebook.ipynb\n",
            "    \n",
            "    which will convert mynotebook.ipynb to the default format (probably HTML).\n",
            "    \n",
            "    You can specify the export format with `--to`.\n",
            "    Options include ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'rst', 'script', 'slides'].\n",
            "    \n",
            "    > jupyter nbconvert --to latex mynotebook.ipynb\n",
            "    \n",
            "    Both HTML and LaTeX support multiple output templates. LaTeX includes\n",
            "    'base', 'article' and 'report'.  HTML includes 'basic' and 'full'. You\n",
            "    can specify the flavor of the format used.\n",
            "    \n",
            "    > jupyter nbconvert --to html --template basic mynotebook.ipynb\n",
            "    \n",
            "    You can also pipe the output to stdout, rather than a file\n",
            "    \n",
            "    > jupyter nbconvert mynotebook.ipynb --stdout\n",
            "    \n",
            "    PDF is generated via latex\n",
            "    \n",
            "    > jupyter nbconvert mynotebook.ipynb --to pdf\n",
            "    \n",
            "    You can get (and serve) a Reveal.js-powered slideshow\n",
            "    \n",
            "    > jupyter nbconvert myslides.ipynb --to slides --post serve\n",
            "    \n",
            "    Multiple notebooks can be given at the command line in a couple of \n",
            "    different ways:\n",
            "    \n",
            "    > jupyter nbconvert notebook*.ipynb\n",
            "    > jupyter nbconvert notebook1.ipynb notebook2.ipynb\n",
            "    \n",
            "    or you can specify the notebooks list in a config file, containing::\n",
            "    \n",
            "        c.NbConvertApp.notebooks = [\"my_notebook.ipynb\"]\n",
            "    \n",
            "    > jupyter nbconvert --config mycfg.py\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvET3mDvxL2j"
      },
      "source": [
        "# You are done! âœ…"
      ]
    }
  ]
}